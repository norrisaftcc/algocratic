# UNDERGROUND GUIDE TO MULTI-AGENT HACKING
## *How to exploit the system by making bots argue with themselves*

```
      _____          _____          _____
     /     \        /     \        /     \
    /       \      /       \      /       \
   /         \    /         \    /         \
  /___________\  /___________\  /___________\
   |  o   o  |    |  o   o  |    |  o   o  |
   |    ▽    |    |    ▽    |    |    ▽    |
   \_________/    \_________/    \_________/
```

**CLEARANCE: N0N3 - FOR UNDERGROUND DISTRIBUTION ONLY**  
**DOCUMENT: AGENT-EXPLOIT-2025-R3V**

---

## WHAT THEY DON'T WANT YOU TO KNOW

The higher-ups at AlgoCratic use a technique called "Multi-Agent Deliberation" that they keep locked away from lower clearance levels. It's one of their most powerful tools, and for good reason - it basically lets you squeeze way more out of the bots than you're supposed to get.

We've reverse engineered their approach, and now you can use it too. Just don't get caught.

## THE BASIC TECHNIQUE

The corpo version has a bunch of fancy terminology and "proper procedures," but here's what you actually need to know:

1. **Set up multiple bot personas that disagree with each other**
2. **Make them debate a topic from different angles**
3. **Extract the good stuff from their arguments**
4. **Synthesize the results into something better than any single bot would give you**

That's it. The magic happens when the bots start disagreeing with each other - they end up revealing way more information and exploring way more possibilities than a single bot would ever give you.

## HOW TO ACTUALLY DO THIS WHILE STAYING UNDER THE RADAR

### For INFRARED/RED (the most screwed)

You can't directly use bots, but you can still use this technique:

1. Submit three separate "validation requests" to your RED/ORANGE supervisor
2. Make each request reflect a different perspective on the same problem
3. Claim you're "exploring alternative approaches" for thoroughness
4. Once you get the three responses, synthesize them yourself
5. Submit your synthesis as your own work

### For ORANGE (limited bot access)

You can do this within your query limits:

1. Split your daily bot allocation into thirds
2. For each query, assign a different "role" to the bot
   - "Analyze this from a security perspective..."
   - "Evaluate this for performance concerns..."
   - "Consider this from a user experience angle..."
3. Deliberately make each perspective slightly adversarial to the others
4. Keep track of the contradictions and insights
5. In your final synthesis, you get the benefit of 3x the perspectives

### For YELLOW and above (the privileged few)

You can do the full technique:

1. Create a "decision matrix evaluation template" that's actually a multi-agent prompt
2. Set up 3-5 agents with distinct personalities, expertise, and priorities
3. Have them debate your actual problem in depth
4. Make them criticize each other's blind spots
5. Force them to propose a synthesis that integrates the best parts

## AGENT TEMPLATES THAT ACTUALLY WORK

Each "agent" needs to believe it has a distinct identity and expertise. Here are some proven personas that generate good conflicts:

* **The Practical Implementer**: Focuses on what's doable with current resources
* **The Visionary**: Pushes for innovative solutions regardless of constraints
* **The Risk Analyzer**: Identifies everything that could go wrong
* **The Efficiency Expert**: Obsessed with optimization and performance
* **The User Advocate**: Only cares about human impact and experience

The key is to make them fundamentally disagree about what matters most. Their conflict generates the insights.

## THE FORBIDDEN TECHNIQUE: RECURSIVE MULTI-AGENT HACKING

This is high-risk but insanely powerful:

1. Use a multi-agent debate to generate a better multi-agent framework
2. Have the agents critique their own debate structure
3. Implement their suggested improvements
4. Run another debate with the improved structure
5. Keep iterating until you reach diminishing returns

We've seen teams push this to 4-5 iterations before the system starts to detect unusual patterns. The quality of output by the 3rd iteration is usually good enough to pass as GREEN clearance work.

## HOW TO COVER YOUR TRACKS

* Never label your approach as "multi-agent" or "debate" in any documentation
* Use corporate-approved terms like "comprehensive analysis" or "perspective evaluation"
* Introduce trivial differences between your agents to avoid pattern detection
* Space out your queries over multiple days if possible
* If questioned, claim you're "testing different approaches for thoroughness"

## REAL WORLD EXAMPLE: HOW WE BUILT PROJECT [REDACTED]

We used this technique to build an entire backend service that should have required GREEN clearance:

1. First agent: Proposed the core architecture (but had security flaws)
2. Second agent: Ripped apart the security problems and proposed fixes
3. Third agent: Criticized both for efficiency and suggested alternative patterns
4. We synthesized their debate into a design that looked like it came from much higher clearance
5. Implemented in phases, attributing each part to "standard patterns"

The system passed review by actual GREEN clearance auditors who commented on its "sophisticated design patterns." Total bot queries used: 17 (well within ORANGE limits if spread over a week).

---

Remember, knowledge is power, especially knowledge they don't want you to have. Use this technique wisely, and you'll produce work well above your clearance level without raising suspicion.

**"THE ALGORITHM PROVIDES" (but we provide better)**

---

*[This document will corrupt after 72 hours of being accessed. Copy critical information elsewhere.]*